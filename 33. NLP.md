Natural Language Processing (NLP) Modules Cheatsheet
Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. Below is a comprehensive cheatsheet covering some of the most popular NLP libraries and modules.
________________________________________
ðŸ”¹ 1. NLTK (Natural Language Toolkit)
Installation:
pip install nltk
Importing NLTK:
import nltk
Downloading NLTK Data:
nltk.download('punkt')  # For tokenization
nltk.download('stopwords')  # For stopwords
Tokenization:
from nltk.tokenize import word_tokenize, sent_tokenize
text = "Hello world! How are you?"
words = word_tokenize(text)
sentences = sent_tokenize(text)
Stopwords Removal:
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]
POS Tagging (Part of Speech Tagging):
from nltk import pos_tag
tags = pos_tag(words)
Named Entity Recognition (NER):
from nltk import ne_chunk
named_entities = ne_chunk(tags)
________________________________________
ðŸ”¹ 2. SpaCy
Installation:
pip install spacy
Importing SpaCy:
import spacy
Loading a Pre-trained Model:
nlp = spacy.load('en_core_web_sm')
Tokenization:
doc = nlp("Hello world! How are you?")
for token in doc:
    print(token.text)
Lemmatization:
for token in doc:
    print(token.lemma())
Named Entity Recognition (NER):
for entity in doc.ents:
    print(entity.text, entity.label_)
POS Tagging:
for token in doc:
    print(token.text, token.pos_)
Sentence Segmentation:
for sent in doc.sents:
    print(sent)
________________________________________
ðŸ”¹ 3. TextBlob
Installation:
pip install textblob
Importing TextBlob:
from textblob import TextBlob
Tokenization:
blob = TextBlob("Hello world! How are you?")
words = blob.words
sentences = blob.sentences
POS Tagging:
tags = blob.tags
Sentiment Analysis:
sentiment = blob.sentiment  # Returns polarity and subjectivity
Lemmatization:
from textblob import Word
word = Word("running")
word.lemmatize("v")  # Verb lemmatization
________________________________________
ðŸ”¹ 4. Gensim
Installation:
pip install gensim
Importing Gensim:
import gensim
Word2Vec Model:
from gensim.models import Word2Vec
sentences = [["this", "is", "a", "sentence"], ["another", "sentence"]]
model = Word2Vec(sentences, min_count=1)
Finding Similar Words:
similar_words = model.wv.most_similar('sentence', topn=5)
Topic Modeling (Latent Dirichlet Allocation):
from gensim import corpora
from gensim.models import LdaModel

texts = [["human", "machine", "interface"], ["machine", "learning"]]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary)
topics = lda_model.print_topics()
________________________________________
ðŸ”¹ 5. Hugging Face Transformers
Installation:
pip install transformers
Importing and Loading Pre-trained Model:
from transformers import pipeline

# Load pre-trained sentiment analysis model
sentiment_analyzer = pipeline("sentiment-analysis")
result = sentiment_analyzer("I love this product!")
Using BERT for Text Classification:
from transformers import BertTokenizer, BertForSequenceClassification
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello, my name is BERT", return_tensors="pt")
outputs = model(**inputs)
Text Generation with GPT:
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")
generated_text = generator("Once upon a time", max_length=50)
________________________________________
ðŸ”¹ 6. Pattern
Installation:
pip install pattern
Importing Pattern:
from pattern.en import sentiment, parse
Sentiment Analysis:
text = "I am happy today!"
polarity, subjectivity = sentiment(text)
Part of Speech Tagging:
parsed = parse("I am learning Python")
print(parsed)
________________________________________
ðŸ”¹ 7. Numpy & Scikit-learn for NLP
TF-IDF Vectorization (Scikit-learn):
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = ["this is the first document", "this document is the second document", "and this is the third one"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print(X.toarray())  # Returns the TF-IDF values
Word Frequency (Using Numpy):
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["apple banana apple", "banana apple orange", "orange orange apple"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
frequencies = np.array(X.sum(axis=0)).flatten()

words = vectorizer.get_feature_names_out()
for word, freq in zip(words, frequencies):
    print(f"{word}: {freq}")
________________________________________
ðŸ”¹ 8. AllenNLP
Installation:
pip install allennlp
Importing AllenNLP:
import allennlp
from allennlp.predictors.predictor import Predictor

# Load a pre-trained model
predictor = Predictor.from_path("https://allennlp.s3.amazonaws.com/models/bert-base-srl-2020.02.20.tar.gz")
result = predictor.predict(sentence="John is eating a sandwich")
print(result)
________________________________________
ðŸ”¹ 9. PyTorch-NLP (Torchtext)
Installation:
pip install torchtext
Importing Torchtext:
import torch
import torchtext
from torchtext.data.utils import get_tokenizer
Tokenization:
tokenizer = get_tokenizer("basic_english")
tokens = tokenizer("This is an example sentence.")
________________________________________
ðŸš€ Key NLP Concepts Covered:
â€¢	Tokenization: Breaking text into words, sentences, or subwords.
â€¢	Stopwords Removal: Filtering out common, unimportant words like "the", "and".
â€¢	Lemmatization: Reducing words to their root form (e.g., "running" â†’ "run").
â€¢	Part-of-Speech (POS) Tagging: Labeling words based on their grammatical role.
â€¢	Named Entity Recognition (NER): Identifying entities (e.g., names, dates) in text.
â€¢	Sentiment Analysis: Detecting the emotional tone of text.
â€¢	Text Classification: Categorizing text into predefined categories.
â€¢	Word Embeddings: Mapping words to dense vectors for better representation (e.g., Word2Vec, GloVe).
â€¢	Sequence-to-Sequence Models: Models that predict output sequences from input sequences (e.g., translation).
â€¢	Attention Mechanisms: Used in modern NLP models to focus on important parts of the input.
________________________________________
This cheatsheet gives you a foundational overview of NLP tasks and tools. You can explore more advanced topics like Transformers, BERT, GPT, etc., for cutting-edge NLP applications!

