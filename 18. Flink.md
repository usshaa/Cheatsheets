Apache Flink Cheatsheet ðŸŒŸ
Apache Flink is a powerful open-source stream processing framework that provides high-throughput, low-latency, and exactly-once processing semantics for both batch and streaming data.
________________________________________
ðŸ”¹ 1. Flink Basics
Flink Stream Processing
â€¢	Stream Processing refers to continuous processing of data in real-time as it arrives.
Flink Batch Processing
â€¢	Batch Processing processes data in chunks or fixed-size batches.
Flink Jobs
â€¢	Flink applications are typically written as a Job, which processes data by applying operators on streams or batch datasets.
// Example of a simple Flink job that reads from a source and prints to a sink
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.fromElements(1, 2, 3, 4, 5)
    .map(value -> value * 2)
    .print();

env.execute("Simple Flink Job");
________________________________________
ðŸ”¹ 2. Environment Setup
Set up Flink StreamExecutionEnvironment
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
â€¢	env.fromElements(): Creates a stream from given elements.
â€¢	env.addSource(): Adds a source like Kafka, File, etc.
â€¢	env.execute(): Executes the Flink job.
________________________________________
ðŸ”¹ 3. Sources and Sinks
Reading Data from a Source
â€¢	From Elements:
DataStream<Integer> dataStream = env.fromElements(1, 2, 3, 4, 5);
â€¢	From File:
DataStream<String> fileStream = env.readTextFile("path/to/input.txt");
â€¢	From Kafka:
FlinkKafkaConsumer<String> kafkaConsumer = new FlinkKafkaConsumer<>(
    "my_topic", new SimpleStringSchema(), properties);
DataStream<String> kafkaStream = env.addSource(kafkaConsumer);
Writing Data to a Sink
â€¢	Print Sink:
dataStream.print();
â€¢	To File:
dataStream.writeAsText("output/path");
â€¢	To Kafka:
FlinkKafkaProducer<String> kafkaProducer = new FlinkKafkaProducer<>(
    "my_output_topic", new SimpleStringSchema(), properties);
dataStream.addSink(kafkaProducer);
________________________________________
ðŸ”¹ 4. Transformations
Map Transformation
// Apply a function on each element
DataStream<Integer> squaredStream = dataStream.map(x -> x * x);
Filter Transformation
// Filter out elements based on a condition
DataStream<Integer> filteredStream = dataStream.filter(x -> x > 2);
KeyBy Transformation
// Group by a key
DataStream<Tuple2<String, Integer>> keyedStream = dataStream.keyBy(0);
Windowing
â€¢	Time Windows: Group elements based on time intervals.
dataStream
    .keyBy(0)
    .timeWindow(Time.seconds(5))
    .sum(1);  // Sum elements within each window
â€¢	Tumbling Windows: Fixed-size time intervals.
dataStream
    .keyBy(0)
    .window(TumblingEventTimeWindows.of(Time.seconds(5)))
    .sum(1);
â€¢	Sliding Windows: Sliding over time.
dataStream
    .keyBy(0)
    .window(SlidingEventTimeWindows.of(Time.seconds(5), Time.seconds(2)))
    .sum(1);
Join Transformation
// Join two streams on key
DataStream<Tuple2<String, Integer>> joinedStream = stream1
    .join(stream2)
    .where(0)
    .equalTo(0)
    .window(TumblingEventTimeWindows.of(Time.seconds(5)))
    .apply(new JoinFunction());
________________________________________
ðŸ”¹ 5. Watermarks & Time Handling
Event Time and Watermarks
â€¢	Watermarks: Used to track the progress of time in event-time processing.
dataStream.assignTimestampsAndWatermarks(
    WatermarkStrategy.<MyEvent>forMonotonousTimestamps()
        .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
);
Processing Time and Ingestion Time
â€¢	Processing Time: System time when events are processed.
â€¢	Ingestion Time: Time when events arrive at the Flink job.
________________________________________
ðŸ”¹ 6. State and Checkpoints
Checkpointing
â€¢	Enabling Checkpoints: Periodically snapshot the state.
env.enableCheckpointing(10000); // 10 seconds
â€¢	State Backend: Configure how state is stored.
env.setStateBackend(new FsStateBackend("file:///checkpoint-dir"));
Savepoints
â€¢	Savepoints are manually triggered checkpoints used for job recovery.
# Trigger savepoint
flink savepoint <jobID> <savepoint-path>
________________________________________
ðŸ”¹ 7. Flink SQL API
Executing SQL Queries
// Define a table
env.executeSql("CREATE TABLE MyTable (id INT, name STRING) WITH ('connector' = 'kafka', 'topic' = 'my_topic')");

// Query the table
Table result = tableEnv.sqlQuery("SELECT id, name FROM MyTable WHERE id > 2");
Using Flink SQL Client
# Start the SQL CLI to interact with Flink
flink sql-client
________________________________________
ðŸ”¹ 8. Flink CEP (Complex Event Processing)
Pattern Detection
// Define a pattern
Pattern<Event, ?> pattern = Pattern.<Event>begin("start")
    .where(event -> event.getValue() > 5)
    .next("end")
    .where(event -> event.getValue() < 3);

// Apply pattern
PatternStream<Event> patternStream = CEP.pattern(dataStream, pattern);
________________________________________
ðŸ”¹ 9. Flink Kafka Integration
Reading from Kafka
FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
    "input-topic", new SimpleStringSchema(), kafkaProperties);
DataStream<String> stream = env.addSource(consumer);
Writing to Kafka
FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(
    "output-topic", new SimpleStringSchema(), kafkaProperties);
dataStream.addSink(producer);
________________________________________
ðŸ”¹ 10. Flink Configuration
Setting Parallelism
// Set global parallelism
env.setParallelism(4);

// Set parallelism per operator
dataStream.setParallelism(2);
Custom Configuration
Configuration config = new Configuration();
config.setString("rest.bind-port", "8081");
StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1, config);
________________________________________
ðŸš€ Flink provides powerful stream processing capabilities with ease of use and scalability, making it ideal for real-time analytics and complex event processing.

