Apache Sqoop Cheatsheet ðŸŒŸ
Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and relational databases. It supports importing data from databases into HDFS and exporting data from HDFS back to relational databases.
________________________________________
ðŸ”¹ 1. Sqoop Basics
Sqoop Import
â€¢	Used to import data from a relational database into Hadoop (HDFS, Hive, HBase).
# Basic Sqoop import from a database to HDFS
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data
Sqoop Export
â€¢	Used to export data from Hadoop (HDFS, Hive, HBase) back into a relational database.
# Basic Sqoop export from HDFS to a database table
sqoop export --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --export-dir /user/hadoop/mytable_data
________________________________________
ðŸ”¹ 2. Data Import Options
Import Specific Columns
# Import only specific columns from a table
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --columns "id,name,email" --target-dir /user/hadoop/mytable_data
Import with Query
# Import data using a custom SQL query
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --query "SELECT id, name FROM mytable WHERE $CONDITIONS" --target-dir /user/hadoop/mytable_data
Import to Hive
# Import directly into a Hive table
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --hive-import --hive-table hive_table_name
Import with Data Splitting
# Split the data into multiple chunks for parallel import
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --split-by id --target-dir /user/hadoop/mytable_data
Import Data with Date Range
# Import data from a date range
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --where "date_column BETWEEN '2021-01-01' AND '2021-12-31'" --target-dir /user/hadoop/mytable_data
________________________________________
ðŸ”¹ 3. Data Export Options
Export Data to Database
# Export data from HDFS to a database table
sqoop export --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --export-dir /user/hadoop/mytable_data
Handling Nulls during Export
# Export data and handle null values
sqoop export --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --export-dir /user/hadoop/mytable_data --null-string '\\N' --null-non-string '\\N'
________________________________________
ðŸ”¹ 4. Advanced Sqoop Operations
Import All Tables
# Import all tables from a database
sqoop import-all-tables --connect jdbc:mysql://localhost/mydatabase --username user --password pass --warehouse-dir /user/hadoop/all_tables_data
Incremental Import
â€¢	Append: Imports only the new records since the last import.
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --incremental append --check-column id --last-value 1000 --target-dir /user/hadoop/mytable_data
â€¢	Lastmodified: Import records that have been modified after a given timestamp.
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --incremental lastmodified --check-column last_modified --last-value '2021-01-01 00:00:00' --target-dir /user/hadoop/mytable_data
Importing to HBase
# Import data from relational database to HBase
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --hbase-table hbase_table_name --column-family cf_name
________________________________________
ðŸ”¹ 5. Performance Tuning
Increasing Parallelism
# Increase the number of mappers for parallel import
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --split-by id --num-mappers 4 --target-dir /user/hadoop/mytable_data
File Formats
# Import data in Avro format
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data --as-avrodatafile

# Import data in Parquet format
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data --as-parquetfile
________________________________________
ðŸ”¹ 6. Sqoop Options
General Options
# Set delimiter for data in import
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data --fields-terminated-by ","

# Specify a custom Java class for importing
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data --map-column-hive 'id=int,name=string'
Compression Options
# Compress the output data
sqoop import --connect jdbc:mysql://localhost/mydatabase --username user --password pass --table mytable --target-dir /user/hadoop/mytable_data --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec
________________________________________
ðŸ”¹ 7. Sqoop Metastore (Optional)
Using Sqoop Metastore
â€¢	Set up and configure a Metastore to store metadata for Sqoop jobs.
# Start the Sqoop metastore service
sqoop metastore start

# Stop the Sqoop metastore service
sqoop metastore stop
________________________________________
ðŸš€ Sqoop is powerful for transferring large volumes of data between Hadoop and relational databases. These commands will help you manage your data movement effectively.

