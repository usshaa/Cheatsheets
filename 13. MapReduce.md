Hadoop MapReduce with Python Cheatsheet ðŸŒŸ
Hadoop MapReduce is a programming model used for processing large datasets in parallel across a distributed Hadoop cluster. While the default language for writing MapReduce jobs is Java, you can also write MapReduce jobs using Python through the Hadoop streaming API. This allows you to use Python to implement both the Mapper and Reducer components of your MapReduce job.
________________________________________
ðŸ”¹ 1. Prerequisites
Before using Python with Hadoop MapReduce, ensure that you have:
1.	Hadoop installed on your system (either locally or on a cluster).
2.	Python installed (generally Python 2.x or Python 3.x).
3.	The Hadoop Streaming API (which allows non-Java applications to communicate with Hadoop).
________________________________________
ðŸ”¹ 2. Hadoop Streaming
Hadoop Streaming is a utility that allows you to use any executable or script as the Mapper and Reducer in a MapReduce job. In Python, we typically use standard input and output to interact with Hadoop.
The basic command syntax for Hadoop streaming is:
hadoop jar /path/to/hadoop-streaming.jar \
  -D mapreduce.job.reduces=1 \
  -input <input_dir> \
  -output <output_dir> \
  -mapper <mapper_script.py> \
  -reducer <reducer_script.py>
Here:
â€¢	-input: The path to the input data in HDFS.
â€¢	-output: The path to store the output in HDFS.
â€¢	-mapper: Path to the Python script that will function as the Mapper.
â€¢	-reducer: Path to the Python script that will function as the Reducer.
________________________________________
ðŸ”¹ 3. Mapper in Python
The Mapper reads input data, processes it, and emits key-value pairs.
Mapper Script (mapper.py)
#!/usr/bin/env python
import sys

# Read each line from the input
for line in sys.stdin:
    line = line.strip()  # Remove leading/trailing whitespace
    words = line.split()  # Split the line into words

    for word in words:
        # Output: word as key, 1 as the value (representing the word count)
        print(f"{word}\t1")
Explanation:
â€¢	sys.stdin: Reads the input line by line.
â€¢	strip(): Removes any unnecessary whitespace.
â€¢	split(): Splits the line into words.
â€¢	print(f"{word}\t1"): Outputs key-value pairs in the format word <tab> 1.
________________________________________
ðŸ”¹ 4. Reducer in Python
The Reducer receives sorted key-value pairs (grouped by the key) and aggregates them.
Reducer Script (reducer.py)
#!/usr/bin/env python
import sys

current_word = None
current_count = 0

# Read from stdin
for line in sys.stdin:
    line = line.strip()  # Strip whitespace
    word, count = line.split("\t")  # Split key and value
    
    try:
        count = int(count)  # Convert count to integer
    except ValueError:
        continue  # Skip any lines that don't have a valid count

    if current_word == word:
        current_count += count  # Add the count for the current word
    else:
        if current_word:
            # Output the final count for the previous word
            print(f"{current_word}\t{current_count}")
        current_word = word  # Set the new word
        current_count = count  # Set the new count

# Output the last word and its count
if current_word == word:
    print(f"{current_word}\t{current_count}")
Explanation:
â€¢	sys.stdin: Reads from the input stream.
â€¢	split("\t"): Splits the key-value pairs.
â€¢	Aggregating counts: The reducer aggregates the counts for each word.
â€¢	print(f"{current_word}\t{current_count}"): Outputs the final word count.
________________________________________
ðŸ”¹ 5. Running the MapReduce Job
1.	Prepare Input Data: Place your input data in HDFS (Hadoop Distributed File System).
hdfs dfs -put /local/input.txt /user/hadoop/input
2.	Run the Hadoop Streaming Command:
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
  -D mapreduce.job.reduces=1 \
  -input /user/hadoop/input \
  -output /user/hadoop/output \
  -mapper mapper.py \
  -reducer reducer.py
3.	Check the Output:
hdfs dfs -cat /user/hadoop/output/part-00000
The output will look like:
word1   count1
word2   count2
word3   count3
________________________________________
ðŸ”¹ 6. Advanced Example: Word Count with Sorting
In Hadoop MapReduce, the key-value pairs emitted by the Mapper are sorted by key before being passed to the Reducer. To sort the results by word frequency, you can use the sort functionality within the reducer or outside of Hadoop.
Mapper (mapper.py)
#!/usr/bin/env python
import sys

for line in sys.stdin:
    line = line.strip()
    words = line.split()

    for word in words:
        print(f"{word}\t1")
Reducer (reducer.py)
#!/usr/bin/env python
import sys
from collections import defaultdict

word_count = defaultdict(int)

for line in sys.stdin:
    line = line.strip()
    word, count = line.split("\t")

    try:
        count = int(count)
    except ValueError:
        continue

    word_count[word] += count

# Sort by word frequency
for word, count in sorted(word_count.items(), key=lambda x: x[1], reverse=True):
    print(f"{word}\t{count}")
________________________________________
ðŸ”¹ 7. Troubleshooting Tips
â€¢	Permission Issues: Ensure that all scripts are executable (chmod +x mapper.py reducer.py).
â€¢	Debugging: Use print() statements inside your Mapper/Reducer scripts for debugging. Redirect stderr to capture errors (2> error.log).
â€¢	Output Path: Ensure the output path does not already exist in HDFS. Use hdfs dfs -rm -r /path/to/output to delete the previous output folder.
â€¢	File Formats: Hadoop assumes the input data is in text format. For other file formats (CSV, JSON, etc.), you may need to use custom serializers or preprocess the data.
________________________________________
ðŸ”¹ 8. Common Errors
â€¢	Missing MapReduce jar: Ensure that Hadoop MapReduce is installed correctly and the jar file is accessible.
â€¢	Invalid Path: Ensure input/output paths in HDFS are correct.
â€¢	Memory Errors: If the job fails due to memory limits, you can adjust configurations like mapreduce.map.memory.mb or mapreduce.reduce.memory.mb.
________________________________________
ðŸš€ Conclusion
Using Python with Hadoop MapReduce through the Hadoop streaming API provides a powerful way to process large datasets. You can implement data processing tasks, such as word counting, log processing, and aggregations, using Python scripts for the Mapper and Reducer. With proper configuration and debugging, Hadoop MapReduce with Python can scale to handle massive datasets in distributed environments.

