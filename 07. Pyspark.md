PySpark Cheatsheet ðŸŒŸ
PySpark is the Python API for Apache Spark, an open-source big data processing framework. PySpark allows Python developers to perform distributed data processing and analytics on large datasets. Below is a comprehensive cheatsheet with key PySpark functionalities, from setting up Spark to various operations on DataFrames.
________________________________________
ðŸ”¹ 1. Setting Up PySpark
Installing PySpark
pip install pyspark
Initializing SparkSession
The SparkSession is the entry point to programming with Spark in PySpark.
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .getOrCreate()
________________________________________
ðŸ”¹ 2. Creating DataFrames
Creating DataFrame from a CSV File
df = spark.read.csv("path_to_file.csv", header=True, inferSchema=True)
df.show()  # Display first few rows
Creating DataFrame from a List
data = [("Alice", 34), ("Bob", 45), ("Charlie", 29)]
columns = ["Name", "Age"]

df = spark.createDataFrame(data, columns)
df.show()
Creating DataFrame from JSON
df_json = spark.read.json("path_to_file.json")
df_json.show()
________________________________________
ðŸ”¹ 3. DataFrame Operations
Select Columns
df.select("Name", "Age").show()
Filtering Data (WHERE Clause)
df.filter(df.Age > 30).show()
Adding New Columns
df = df.withColumn("Senior", df.Age > 40)
df.show()
Renaming Columns
df = df.withColumnRenamed("Name", "Full_Name")
df.show()
Dropping Columns
df = df.drop("Age")
df.show()
Sorting Data
df.orderBy("Age", ascending=False).show()
________________________________________
ðŸ”¹ 4. Aggregations and Grouping
Group By and Aggregate Functions
from pyspark.sql import functions as F

df.groupBy("Age").agg(F.count("Name").alias("Count")).show()
Using Multiple Aggregation Functions
df.groupBy("Age").agg(F.mean("Age").alias("Average_Age"), F.max("Age").alias("Max_Age")).show()
Counting Distinct Values
df.select("Age").distinct().show()
________________________________________
ðŸ”¹ 5. Working with Null Values
Check for Null Values
df.filter(df.Age.isNull()).show()
Filling Null Values
df = df.fillna({"Age": 0, "Name": "Unknown"})
df.show()
Dropping Rows with Null Values
df = df.dropna()
df.show()
________________________________________
ðŸ”¹ 6. Joins
Inner Join
df1 = spark.createDataFrame([("Alice", 1), ("Bob", 2)], ["Name", "ID"])
df2 = spark.createDataFrame([("Alice", "HR"), ("Bob", "IT")], ["Name", "Department"])

df_joined = df1.join(df2, on="Name", how="inner")
df_joined.show()
Left Outer Join
df_left_join = df1.join(df2, on="Name", how="left")
df_left_join.show()
Right Outer Join
df_right_join = df1.join(df2, on="Name", how="right")
df_right_join.show()
________________________________________
ðŸ”¹ 7. Spark SQL
Running SQL Queries
df.createOrReplaceTempView("people")
result = spark.sql("SELECT Name, Age FROM people WHERE Age > 30")
result.show()
________________________________________
ðŸ”¹ 8. File Formats
Reading Parquet Files
df_parquet = spark.read.parquet("path_to_file.parquet")
df_parquet.show()
Writing DataFrames to CSV
df.write.csv("output_file.csv", header=True)
Writing DataFrames to Parquet
df.write.parquet("output_file.parquet")
________________________________________
ðŸ”¹ 9. UDF (User Defined Functions)
Defining a UDF
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Define a UDF
def upper_case(name):
    return name.upper()

# Register UDF
udf_upper_case = udf(upper_case, StringType())

# Apply UDF to a DataFrame
df_with_upper = df.withColumn("Upper_Name", udf_upper_case(df.Name))
df_with_upper.show()
Using PySpark Built-In Functions
# Using built-in functions for transformation
df.withColumn("Name_Length", F.length(df.Name)).show()
________________________________________
ðŸ”¹ 10. RDD (Resilient Distributed Datasets)
Creating an RDD
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd.collect()  # Returns the list: [1, 2, 3, 4, 5]
RDD Transformation: map()
rdd = rdd.map(lambda x: x * 2)
rdd.collect()  # Returns the list: [2, 4, 6, 8, 10]
RDD Action: reduce()
sum_rdd = rdd.reduce(lambda a, b: a + b)
print(sum_rdd)  # Returns the sum of the list: 30
RDD Action: filter()
filtered_rdd = rdd.filter(lambda x: x > 5)
filtered_rdd.collect()  # Returns the list: [6, 8, 10]
________________________________________
ðŸ”¹ 11. Caching and Persisting
Caching a DataFrame
df.cache()
Persisting a DataFrame with Different Storage Levels
from pyspark import StorageLevel

df.persist(StorageLevel.MEMORY_AND_DISK)
________________________________________
ðŸ”¹ 12. PySpark MLlib (Machine Learning)
VectorAssembler (Feature Engineering)
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["Age"], outputCol="features")
df = assembler.transform(df)
df.show()
Logistic Regression Example
from pyspark.ml.classification import LogisticRegression

# Train a model
lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(df)

# Make predictions
predictions = model.transform(df)
predictions.show()
________________________________________
ðŸ”¹ 13. Monitoring and Debugging
View Spark UI
Access the Spark UI at http://<driver-node>:4040 to see the job's stages, tasks, and storage information.
________________________________________
ðŸ”¹ 14. PySpark Cluster Configurations
Configuring Spark Session with More Options
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.executor.memory", "2g") \
    .config("spark.cores.max", "4") \
    .getOrCreate()
________________________________________
ðŸ”¹ 15. Closing Spark Session
spark.stop()
________________________________________
Summary of Common PySpark Functions
Function	Description
spark.read.csv()	Read CSV files into DataFrame
df.show()	Display the DataFrame
df.filter()	Filter rows based on condition
df.select()	Select columns from the DataFrame
df.groupBy()	Group DataFrame by specified column(s)
df.agg()	Apply aggregation functions (e.g., mean, sum)
df.withColumn()	Add a new column or modify an existing one
spark.sql()	Execute SQL queries on DataFrames
df.write.csv()	Write DataFrame to a CSV file
VectorAssembler	Combine multiple columns into a feature vector
________________________________________
ðŸ”¥ With PySpark, you can efficiently process and analyze large datasets in a distributed manner, whether you're working on data transformation, machine learning, or analytics! ðŸš€

