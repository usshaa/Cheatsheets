Kafka Cheatsheet ðŸŒŸ
Apache Kafka is a distributed event streaming platform primarily used for building real-time data pipelines and streaming applications. It allows you to publish, subscribe to, store, and process streams of records in real-time.
________________________________________
ðŸ”¹ 1. Kafka Basics
Key Concepts
â€¢	Producer: The entity that sends data (events/messages) to a Kafka topic.
â€¢	Consumer: The entity that reads data from a Kafka topic.
â€¢	Topic: A category or feed to which messages are sent by producers.
â€¢	Partition: A topic is split into partitions to allow for parallel processing. Each partition is an ordered, immutable sequence of messages.
â€¢	Broker: A Kafka server that stores messages in partitions and serves requests from producers and consumers.
â€¢	ZooKeeper: A distributed coordination service used by Kafka for managing cluster metadata and leader election (Note: Kafka is moving away from ZooKeeper in newer versions).
â€¢	Consumer Group: A group of consumers that work together to consume data from a topic. Each consumer in a group reads from a different partition.
________________________________________
ðŸ”¹ 2. Kafka Architecture
â€¢	Producer sends messages to a Topic.
â€¢	Kafka Broker stores these messages in Partitions.
â€¢	Consumers subscribe to the Topic and read messages from the partitions.
â€¢	ZooKeeper (deprecated in newer versions) manages metadata and cluster coordination.
Kafkaâ€™s architecture is designed to allow high throughput, scalability, and fault tolerance. Each partition can be replicated to multiple brokers for high availability.
________________________________________
ðŸ”¹ 3. Kafka Setup
Kafka Installation (Linux Example)
1.	Download Kafka:
2.	wget https://downloads.apache.org/kafka/<version>/kafka_2.13-<version>.tgz
3.	tar -xvzf kafka_2.13-<version>.tgz
4.	cd kafka_2.13-<version>
5.	Start Kafka (requires Zookeeper running first):
6.	# Start Zookeeper
7.	bin/zookeeper-server-start.sh config/zookeeper.properties
8.	
9.	# Start Kafka Broker
10.	bin/kafka-server-start.sh config/server.properties
________________________________________
ðŸ”¹ 4. Kafka Commands
Kafka Producer Commands
â€¢	Produce a message to a topic:
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic <topic_name>
â€¢	Send a message: 
â€¢	Hello Kafka!
Kafka Consumer Commands
â€¢	Consume messages from a topic:
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic_name> --from-beginning
Create a Topic
bin/kafka-topics.sh --create --topic <topic_name> --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
List Topics
bin/kafka-topics.sh --list --bootstrap-server localhost:9092
Describe a Topic
bin/kafka-topics.sh --describe --topic <topic_name> --bootstrap-server localhost:9092
Delete a Topic
bin/kafka-topics.sh --delete --topic <topic_name> --bootstrap-server localhost:9092
________________________________________
ðŸ”¹ 5. Kafka Producer Example (Java)
import org.apache.kafka.clients.producer.*;

import java.util.Properties;

public class KafkaProducerExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        Producer<String, String> producer = new KafkaProducer<>(props);
        ProducerRecord<String, String> record = new ProducerRecord<>("my-topic", "key", "value");

        try {
            producer.send(record, new Callback() {
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    if (exception != null) {
                        exception.printStackTrace();
                    } else {
                        System.out.println("Sent message: " + metadata);
                    }
                }
            });
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.close();
        }
    }
}
________________________________________
ðŸ”¹ 6. Kafka Consumer Example (Java)
import org.apache.kafka.clients.consumer.*;

import java.util.Properties;
import java.util.Arrays;

public class KafkaConsumerExample {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("my-topic"));

        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000);
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Consumed record: Key = %s, Value = %s%n", record.key(), record.value());
                }
            }
        } finally {
            consumer.close();
        }
    }
}
________________________________________
ðŸ”¹ 7. Kafka Consumer Groups
â€¢	What is a Consumer Group? 
o	A consumer group allows multiple consumers to share the work of consuming messages from a topic.
o	Each consumer in the group reads from a different partition, enabling parallel processing.
Example: Multiple Consumers in a Group
# Consumer 1
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic_name> --group <group_id>

# Consumer 2
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic_name> --group <group_id>
Both consumers will share the topic partitions.
________________________________________
ðŸ”¹ 8. Kafka Producer Configurations
â€¢	acks: Determines how many acknowledgments the producer requires before considering a request complete. Options:
o	0: No acknowledgment.
o	1: Wait for leader acknowledgment.
o	all: Wait for acknowledgment from all replicas.
â€¢	batch.size: The size of the batch of messages to be sent in one request.
â€¢	linger.ms: Time to wait before sending a batch of messages.
Example Producer Configuration:
props.put("acks", "all");
props.put("batch.size", 16384);  // 16KB
props.put("linger.ms", 1);  // Milliseconds to wait before sending a batch
________________________________________
ðŸ”¹ 9. Kafka Consumer Configurations
â€¢	auto.offset.reset: What to do when there is no initial offset or if the offset is out of range.
o	earliest: Start from the earliest message.
o	latest: Start from the latest message.
â€¢	enable.auto.commit: If true, the consumer's offset will be periodically committed.
Example Consumer Configuration:
props.put("auto.offset.reset", "earliest");
props.put("enable.auto.commit", "true");
________________________________________
ðŸ”¹ 10. Kafka Stream Processing
Kafka Streams is a client library for building applications and microservices that process data stored in Kafka topics. It simplifies stream processing and supports operations like filtering, aggregation, and windowing.
â€¢	Stream Processing Example (Java):
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> stream = builder.stream("input-topic");
KStream<String, String> filtered = stream.filter((key, value) -> value.contains("error"));
filtered.to("error-topic");

KafkaStreams streams = new KafkaStreams(builder.build(), config);
streams.start();
________________________________________
ðŸ”¹ 11. Kafka Topics and Partitions
â€¢	Topic: Logical channel to which producers write and consumers read.
â€¢	Partition: A topic can have multiple partitions, each of which is an ordered, immutable sequence of messages.
Topic Example:
â€¢	You can create topics with specific numbers of partitions to control parallelism. 
â€¢	bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
________________________________________
ðŸ”¹ 12. Kafka Connect
Kafka Connect is a framework to integrate Kafka with external systems such as databases, file systems, and other applications.
â€¢	Example: Kafka Connect JDBC Source Connector (for MySQL): 
â€¢	{
â€¢	  "name": "jdbc-source-connector",
â€¢	  "config": {
â€¢	    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
â€¢	    "tasks.max": "1",
â€¢	    "connection.url": "jdbc:mysql://localhost:3306/mydb",
â€¢	    "query": "SELECT * FROM users",
â€¢	    "topic.prefix": "mysql-"
â€¢	  }
â€¢	}
________________________________________
ðŸš€ Kafka is an incredibly powerful tool for building real-time data pipelines, and its ability to handle high-throughput, distributed messaging makes it ideal for streaming applications, log aggregation, and more.

